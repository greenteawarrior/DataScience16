{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from the Ground Up\n",
    "\n",
    "Today we will be creating a machine learning algorithm without relying on anything more than code we write ourselves and some basic calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Least Squares\n",
    "\n",
    "All of the theory we just developed can be applied to the case where we have multiple independent variables that we'd like to use to model a single dependent variable.  In this case, we assume that the datapoints are now given as $x_1, \\ldots, x_n$ with $x_i \\in \\mathbb{R}^d$.  As before we are also given corresponding targets $y_1, \\ldots, y_n$ with $y_i \\in \\mathbb{R}$.  Now, the relationship between $y_i$ and $x_i$ is modeled as:\n",
    "\n",
    "$y_i = \\sum_{j=1}^d w_jx_i + \\epsilon$\n",
    "\n",
    "### Computing the Best Weights\n",
    "\n",
    "In order to compute the best weights we have to define what we mean by best.  We will use the exact same notion of \"error\" that we used for the single variable case, that is the sum of squared errors.  To make this more concrete, let's load a dataset with multiple variables that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'smile_dataset.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c91176f695b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_smiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/greenteawarrior/DataScience16/inclass/day06/load_smiles.py\u001b[0m in \u001b[0;36mload_smiles\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_smiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msmiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'smile_dataset.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mreturn_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mreturn_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m    124\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mMR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[1;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m        \u001b[0mtype\u001b[0m \u001b[0mdetected\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mbyte_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'.mat'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# not a string - maybe file-like object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'smile_dataset.mat'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from load_smiles import load_smiles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_smiles(images, targets):\n",
    "    \"\"\" Adapted from Jake Vanderplas' scikit learn tutorials. \"\"\"\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(24, 24))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i,:-1].reshape((24,24)).T, cmap='gray')\n",
    "        ax.text(0.05, 0.05, str(targets[i]),\n",
    "                transform=ax.transAxes,\n",
    "                color='green' if (targets[i]) else 'red')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "data = load_smiles()\n",
    "X, y = data.data, data.target\n",
    "X = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "show_smiles(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Error Function\n",
    "\n",
    "Write some code to calculate the error for a set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_multi(w, X, y):\n",
    "    predictions = X.dot(w)\n",
    "    squared_errors = (predictions - y)**2\n",
    "    return np.sum(squared_errors)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Gradient\n",
    "\n",
    "The gradient of a function is a vector of all of the function's partial derivatives.\n",
    "\n",
    "$\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}f(x) \\\\ \\frac{\\partial}{\\partial x_2}f(x) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_d}f(x) \\end{bmatrix}$\n",
    "\n",
    "Next, we will write a function to compute the gradient of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_error_multi(w, X, y):\n",
    "    \"\"\" Compute the gradient of error_multi with respect to w\n",
    "        in a more efficient manner. \"\"\"\n",
    "    grad = np.zeros(w.shape)\n",
    "    res = X.dot(w) - y\n",
    "    grad = 2*res.T.dot(X).T\n",
    "    return grad\n",
    "\n",
    "w = np.ones((X.shape[1],1))\n",
    "\n",
    "e_0 = np.zeros(w.shape)\n",
    "e_0[0,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 0\", (error_multi(w+e_0, X, y) - error_multi(w, X, y))/e_0[0,0]\n",
    "print \"Analytical partial 0\", grad_error_multi(w, X, y)[0]\n",
    "\n",
    "e_1 = np.zeros(w.shape)\n",
    "e_1[1,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 1\", (error_multi(w+e_1, X, y) - error_multi(w, X, y))/e_1[1,0]\n",
    "print \"Analytical partial 1\", grad_error_multi(w, X, y)[1]\n",
    "\n",
    "e_2 = np.zeros(w.shape)\n",
    "e_2[2,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 1\", (error_multi(w+e_2, X, y) - error_multi(w, X, y))/e_2[2,0]\n",
    "print \"Analytical partial 1\", grad_error_multi(w, X, y)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_multi(w, x, y, step_size, iters):\n",
    "    \"\"\" Perform `iters` iterations of gradient descent\n",
    "        step_size is the step size to start with (this will be adapted)\n",
    "        w is a dx1 numpy array containing an initial guess for the parameters\n",
    "        x is a nxd numpy array containing the independent variables\n",
    "        y is a nx1 numpy array containing the dependent variable \n",
    "        \n",
    "        returns: the fitted value of w, the error for each iteration,\n",
    "                 and a list containing the value of w at each iteration. \"\"\"\n",
    "    errors = np.zeros((iters,1))\n",
    "    last_error = error_multi(w, x, y)\n",
    "    all_ws = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        all_ws.append(w)\n",
    "        grad = grad_error_multi(w, x, y)\n",
    "        w_proposed = w - step_size*grad\n",
    "        error_proposed = error_multi(w_proposed, x, y)\n",
    "        if error_proposed < last_error:\n",
    "            last_error = error_proposed\n",
    "            w = w_proposed\n",
    "            step_size *= 1.1\n",
    "        else:\n",
    "            step_size *= 0.8\n",
    "        if i % 100 == 0:\n",
    "            print \"iter\", i, \"error\", last_error\n",
    "        errors[i] = last_error\n",
    "    return w, errors, all_ws\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.5)\n",
    "w = np.zeros(w.shape)\n",
    "w_f, errors, all_ws = gradient_descent_multi(w,\n",
    "                                             X_train,\n",
    "                                             y_train,\n",
    "                                             10**-7,\n",
    "                                             1000)\n",
    "plt.plot(errors)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do even a little bit better if we use the rmsprop method for optimization.  Here, we have a per parameter step size.  We test whether or not a proposed value of w has the same sign for a specific component of the gradient as the previous value.  If the signs are the same, we get more aggressive with our learning rate for that parameter.  If the signs differ, we become more conservative and refuse the update to that particular parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rprop_descent_multi(w, x, y, step_size, iters):\n",
    "    \"\"\" Perform `iters` iterations of rprop https://en.wikipedia.org/wiki/Rprop\n",
    "        step_size is the step size to start with (this will be adapted per feature)\n",
    "        w is a dx1 numpy array containing an initial guess for the parameters\n",
    "        x is a nxd numpy array containing the independent variables\n",
    "        y is a nx1 numpy array containing the dependent variable \n",
    "        \n",
    "        returns: the fitted value of w, the error for each iteration,\n",
    "                 and a list containing the value of w at each iteration. \"\"\"\n",
    "    g = step_size*np.ones(w.shape)\n",
    "    errors = np.zeros((iters,1))\n",
    "    last_error = error_multi(w, x, y)\n",
    "    all_ws = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        all_ws.append(np.copy(w))\n",
    "        grad = grad_error_multi(w, x, y)\n",
    "        w_proposed = w - g*grad\n",
    "        grad_proposed = grad_error_multi(w_proposed, x, y)\n",
    "        mask = np.sign(grad) == np.sign(grad_proposed)\n",
    "        w[mask] = w_proposed[mask]\n",
    "        g[mask] *= 1.1\n",
    "        g[~mask] *= 0.8\n",
    "\n",
    "        errors[i] = error_multi(w, x, y)\n",
    "        if i % 100 == 0:\n",
    "            print \"iter\", i, \"error\", errors[i]\n",
    "    return w, errors, all_ws\n",
    "\n",
    "w = np.zeros(w.shape)\n",
    "w_f_rprop, errors_rprop, all_ws = rprop_descent_multi(w,\n",
    "                                                      X_train,\n",
    "                                                      y_train,\n",
    "                                                      10**-6.5,\n",
    "                                                      1000)\n",
    "plt.plot(errors)\n",
    "plt.plot(errors_rprop)\n",
    "plt.legend(['Vanilla GD', 'rprop'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sequences(T):\n",
    "    \"\"\" Create the sequences needed for Nesterov's accelerated\n",
    "        gradient descent. \"\"\"\n",
    "    lambdas = np.zeros((T,))\n",
    "    gammas = np.zeros((T-1,))\n",
    "    lambdas[0] = 0.0\n",
    "    for t in range(1,T):\n",
    "        lambdas[t] = (1. + (1 + 4*lambdas[t-1]**2)**0.5)/2.0\n",
    "        gammas[t-1] = (1 - lambdas[t-1])/lambdas[t]\n",
    "    return lambdas, gammas\n",
    "\n",
    "def nesterov_descent_multi(w, x, y, beta, iters):\n",
    "    \"\"\" Perform `iters` iterations of Nesterov accelerated gradient descent\n",
    "        https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/\n",
    "        The parameter beta can be thought of\n",
    "        as 1 over the step size.\n",
    "        w is a dx1 numpy array containing an initial guess for the parameters\n",
    "        x is a nxd numpy array containing the independent variables\n",
    "        y is a nx1 numpy array containing the dependent variable \n",
    "        \n",
    "        returns: the fitted value of w, the error for each iteration,\n",
    "                 and a list containing the value of w at each iteration. \"\"\"\n",
    "    lambdas, gammas = make_sequences(iters+1)\n",
    "    errors = np.zeros((iters,1))\n",
    "    all_ws = []\n",
    "    all_ys = [np.copy(w)]\n",
    "\n",
    "    for i in range(iters):\n",
    "        all_ws.append(np.copy(w))\n",
    "        grad = grad_error_multi(w, x, y)\n",
    "        y_s = w - 1./beta*grad\n",
    "        w = (1 - gammas[i])*y_s + gammas[i]*all_ys[-1]\n",
    "        all_ys.append(np.copy(y_s))\n",
    "\n",
    "        errors[i] = error_multi(w, x, y)\n",
    "        if i % 100 == 0:\n",
    "            print \"iter\", i, \"error\", errors[i]\n",
    "    return w, errors, all_ws\n",
    "\n",
    "w = np.zeros(w.shape)\n",
    "w_f_nesterov, errors_nesterov, all_ws = nesterov_descent_multi(w,\n",
    "                                                               X_train,\n",
    "                                                               y_train,\n",
    "                                                               10**6.45,\n",
    "                                                               1000)\n",
    "plt.plot(errors)\n",
    "plt.plot(errors_rprop)\n",
    "plt.plot(errors_nesterov)\n",
    "plt.legend(['Vanilla GD', 'rprop', 'Nesterov Accelerated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conjugate_descent_multi(w, x, y, iters):\n",
    "    \"\"\" Perform `iters` iterations of conjugate gradient descent\n",
    "        https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "        w is a dx1 numpy array containing an initial guess for the parameters\n",
    "        x is a nxd numpy array containing the independent variables\n",
    "        y is a nx1 numpy array containing the dependent variable \n",
    "        \n",
    "        returns: the fitted value of w, the error for each iteration,\n",
    "                 and a list containing the value of w at each iteration. \"\"\"\n",
    "    errors = np.zeros((iters,1))\n",
    "    A = x.T.dot(x)\n",
    "    b = x.T.dot(y)\n",
    "    r = b - A.dot(w)\n",
    "    p = np.copy(r)\n",
    "    k = 0\n",
    "    for i in range(iters):\n",
    "        alpha_k = r.T.dot(r) / (p.T.dot(A).dot(p))\n",
    "        if alpha_k == 0:\n",
    "            errors[i:] = errors[i-1,0]\n",
    "            break\n",
    "        w = w + alpha_k*p\n",
    "        r_old = np.copy(r)\n",
    "        r = r - alpha_k*A.dot(p)\n",
    "        beta_k = (r.T.dot(r))/(r_old.T.dot(r_old))\n",
    "        p = r + beta_k*p\n",
    "        k = k + 1\n",
    "        errors[i] = error_multi(w, x, y)\n",
    "        if i % 100 == 0:\n",
    "            print \"iter\", i, \"error\", errors[i]\n",
    "    return w, errors, all_ws\n",
    "\n",
    "w = np.zeros(w.shape)\n",
    "w_f_conjugate, errors_conjugate, all_ws = conjugate_descent_multi(w,\n",
    "                                                                  X_train,\n",
    "                                                                  y_train,\n",
    "                                                                  1000)\n",
    "plt.plot(errors)\n",
    "plt.plot(errors_rprop)\n",
    "plt.plot(errors_nesterov)\n",
    "plt.plot(errors_conjugate)\n",
    "plt.legend(['Vanilla GD', 'rprop', 'Nesterov Accelerated', 'Conjugate Gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "print \"Optimal error\", np.sum((model.predict(X_train) - y_train)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"adaptive\", np.mean((X_test.dot(w_f) > 0.5) == y_test)\n",
    "print \"rmsprop\", np.mean((X_test.dot(w_f_rprop) > 0.5) == y_test)\n",
    "print \"nesterov\", np.mean((X_test.dot(w_f_nesterov) > 0.5) == y_test)\n",
    "print \"conjugate\", np.mean((X_test.dot(w_f_conjugate) > 0.5) == y_test)\n",
    "print \"sklearn\", np.mean((model.predict(X_test) > 0.5) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!!!\n",
    "\n",
    "Unfortunately, we don't always have the luxury of having this much labeled data.  Let's create a learning curve that plots the performance of the model on test data as a function of the proportion used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_props = np.arange(.01,.15, .05)\n",
    "n_trials = 10\n",
    "accuracies = np.zeros((train_props.shape[0], n_trials))\n",
    "for i in range(train_props.shape[0]):\n",
    "    print \"Testing proportion %f\" % (train_props[i],)\n",
    "    for j in range(n_trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            train_size=train_props[i],\n",
    "                                                            random_state = j)\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(X_train, y_train)\n",
    "        accuracies[i,j] = np.mean((model.predict(X_test) > 0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_props, accuracies.mean(axis=1),'b.')\n",
    "plt.xlabel('proportion of data used for training')\n",
    "plt.ylabel('model accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very poor performance for low amounts of training data is unfortunately.  However, there are ways that we can make progress.\n",
    "\n",
    "First, we will need to think more about the problem we are solving. Let's examine what happens when we have very little training data (say $1\\%$).  How many parameters are we fitting, how many data points do we have?  What do you suppose the error of this model is on the training set?  Once your done thinking this over, we will verify your conclusions by showing the model predictions versus the smile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to write together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Next, we will learn about a technique called [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) that can greatly help with the problem identified above.\n",
    "\n",
    "The basic idea is to modify our notion of the best set of weights from simply considering the sum of squared errors to also considering the values of the weights themselves.  Here is what our error function looked like before.\n",
    "\n",
    "$e(w) = \\sum_{i=1}^n \\left ( y_i - \\sum_{j=1}^d x_{i,j} w_j \\right )^2$\n",
    "\n",
    "This works really well when we have lots of data compared to the number of free parameters ($n \\gg d$).  In order to handle situations where we don't have very much data compared to the number of parameters, we can modify our error function above in a small, but very important way.\n",
    "\n",
    "$e_\\alpha(w) = \\alpha \\sum_{i=1}^d w_i^2 + \\sum_{i=1}^n \\left ( y_i - \\sum_{j=1}^d x_{i,j} w_j \\right )^2$\n",
    "\n",
    "Where $\\alpha \\in \\mathbb{R}^+$ is a parameter that must be set.  This new error function controls the degree to which the learning algorithm should match its predictions to the training data versus keep the weights low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Implementation\n",
    "\n",
    "Let's modify the functions that we wrote for the error and the gradient to allow for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_multi_alpha(w, X, y, alpha):\n",
    "    \"\"\" Compute error with regularization strength alpha \"\"\"\n",
    "    predictions = X.dot(w)\n",
    "    squared_errors = (predictions - y)**2\n",
    "    return np.sum(squared_errors) + alpha*np.sum(w**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_error_multi_alpha(w, X, y, alpha):\n",
    "    \"\"\" Compute the gradient of the error for the specified\n",
    "        regularization strength alpha \"\"\"\n",
    "    grad = np.zeros(w.shape)\n",
    "    res = X.dot(w) - y\n",
    "    return 2*res.T.dot(X).T + 2*alpha*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our standard sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.ones((X.shape[1],1))\n",
    "\n",
    "alpha = 50\n",
    "\n",
    "e_0 = np.zeros(w.shape)\n",
    "e_0[0,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 0\", (error_multi_alpha(w+e_0, X, y, alpha) -\n",
    "                             error_multi_alpha(w, X, y, alpha))/e_0[0,0]\n",
    "print \"Analytical partial 0\", grad_error_multi_alpha(w, X, y, alpha)[0]\n",
    "\n",
    "e_1 = np.zeros(w.shape)\n",
    "e_1[1,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 1\", (error_multi)alpha(w+e_1, X, y, alpha) -\n",
    "                             error_multi_alpha(w, X, y, alpha))/e_1[1,0]\n",
    "print \"Analytical partial 1\", grad_error_multi(w, X, y)[1]\n",
    "\n",
    "e_2 = np.zeros(w.shape)\n",
    "e_2[2,0] = 10**-5\n",
    "\n",
    "print \"Computed partial 1\", (error_multi(w+e_2, X, y) - error_multi(w, X, y))/e_2[2,0]\n",
    "print \"Analytical partial 1\", grad_error_multi(w, X, y)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify our learning algorithm to work with our new gradient function very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nesterov_descent_multi_alpha(w, x, y, beta, alpha, iters):\n",
    "    \"\"\" Perform `iters` iterations of Nesterov accelerated gradient descent\n",
    "        https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/\n",
    "        The parameter beta can be thought of\n",
    "        as 1 over the step size.\n",
    "        alpha is the strength of the regularization.\n",
    "        w is a dx1 numpy array containing an initial guess for the parameters\n",
    "        x is a nxd numpy array containing the independent variables\n",
    "        y is a nx1 numpy array containing the dependent variable \n",
    "        \n",
    "        returns: the fitted value of w, the error for each iteration,\n",
    "                 and a list containing the value of w at each iteration. \"\"\"\n",
    "    lambdas, gammas = make_sequences(iters+1)\n",
    "    errors = np.zeros((iters,1))\n",
    "    all_ws = []\n",
    "    all_ys = [np.copy(w)]\n",
    "\n",
    "    for i in range(iters):\n",
    "        all_ws.append(np.copy(w))\n",
    "        grad = grad_error_multi_alpha(w, x, y, alpha)\n",
    "        y_s = w - 1./beta*grad\n",
    "        w = (1 - gammas[i])*y_s + gammas[i]*all_ys[-1]\n",
    "        all_ys.append(np.copy(y_s))\n",
    "\n",
    "        errors[i] = error_multi_alpha(w, x, y, alpha)\n",
    "    return w, errors, all_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test how well our algorithm does when given very small amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_props = np.arange(.01,.15, .05)\n",
    "n_trials = 10\n",
    "accuracies = np.zeros((train_props.shape[0], n_trials))\n",
    "for i in range(train_props.shape[0]):\n",
    "    print \"Testing proportion %f\" % (train_props[i],)\n",
    "    for j in range(n_trials):\n",
    "        w = np.zeros((X.shape[1],1))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            train_size=train_props[i],\n",
    "                                                            random_state = j)\n",
    "        w_f, _, _ = nesterov_descent_multi_alpha(w, X_train, y_train, 10**6.45, 50, 1000)\n",
    "        accuracies[i,j] = np.mean((X_test.dot(w_f) > 0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_props, accuracies.mean(axis=1),'b.')\n",
    "plt.xlabel('proportion of data used for training')\n",
    "plt.ylabel('model accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right value for $\\alpha$\n",
    "\n",
    "This looks like it helped quite a bit for $6\\%$ and $11\\%$ of the training data, but perhaps not as dramatically as we might have wanted for $1\\%$.  This is where we face to question: \"How can one tune a machine learning algorithm?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come up with some ways that you might use to set the value of $\\alpha$.  These could be methods that are specific to regularized linear regression or methods that can work for any machine learning algorithm that has a tunable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provdies some [beautiful methods](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) for solving this problem in a principled fashion.  Take some time to investigate these methods.  Try them out on the smile dataset and see if you can improve the performance of the model when faced with a low number of traning examples.  Scikit-learn also provides a model sklearn.linear_model.Ridge that supports regularization (so we don't necessarily have to use our own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
